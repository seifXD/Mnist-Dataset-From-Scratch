# Mnist Dataset From Scratch (No Pytorch, TensorFlow or keras)
# MNIST Digit Classifier from Scratch

This project implements a simple feedforward neural network from scratch to classify handwritten digits from the MNIST dataset. It avoids using high-level machine learning libraries like PyTorch or TensorFlow, relying instead on Python and NumPy. The goal is to show a clear understanding of how neural networks work under the hood.

---

## ğŸ§  Project Overview

The neural network is trained to classify digits (0â€“9) from 28x28 grayscale images. Each image is flattened into a 784-dimensional vector (28 Ã— 28) and passed through a fully connected feedforward neural network.

---

## ğŸ—ï¸ Neural Network Architecture

- **Input Layer:** 784 neurons (one per pixel)
- **Hidden Layer(s):** Configurable size (e.g., 128 or 64 neurons)
- **Output Layer:** 10 neurons (digits 0 through 9)

Each layer is fully connected to the next.

---

## ğŸ” Activation Functions

### ReLU (Rectified Linear Unit)

Used in hidden layers to introduce non-linearity:

```
ReLU(z) = max(0, z)
```

Its derivative (used in backpropagation):

```
ReLU'(z) = 1 if z > 0 else 0
```

### Softmax

Used at the output layer to produce a probability distribution over the digit classes:

```
softmax(z_i) = e^(z_i) / Î£ e^(z_j)
```

---

## ğŸ”„ Forward Propagation

1. **Linear Transformation:**

```
z(l) = W(l) * a(l-1) + b(l)
```

2. **Activation:**

```
a(l) = ReLU(z(l))    # for hidden layers
a(L) = softmax(z(L)) # for output layer
```

This process continues layer by layer until the final output is produced.

---

## ğŸ“‰ Loss Function: Cross-Entropy

The cross-entropy loss measures how well the predicted output matches the true label:

```
L = -Î£ y_i * log(Å·_i)
```

Where:
- `y_i` is the true label (one-hot encoded)
- `Å·_i` is the predicted probability for class `i`

---

## ğŸ” Backpropagation

Used to calculate gradients of the loss with respect to the weights and biases.

1. **Output Error:**

```
Î´(L) = Å· - y
```

2. **Backpropagation Through Hidden Layers:**

```
Î´(l) = (W(l+1)^T) * Î´(l+1) * ReLU'(z(l))
```

3. **Gradient Calculation:**

```
âˆ‚L/âˆ‚W(l) = Î´(l) * (a(l-1))^T
âˆ‚L/âˆ‚b(l) = Î´(l)
```

---

## ğŸ§® Parameter Updates (Gradient Descent)

Weights and biases are updated using gradient descent:

```
W(l) := W(l) - Î· * âˆ‚L/âˆ‚W(l)
b(l) := b(l) - Î· * âˆ‚L/âˆ‚b(l)
```

Where `Î·` is the learning rate.

---

## ğŸƒâ€â™‚ï¸ Training Process

The model is trained over several epochs. After each epoch, the model's performance is evaluated (typically using accuracy on a validation set) to track learning progress and check for overfitting.

---

## âœ… Evaluation Metric

- **Accuracy:** The percentage of correctly classified digits.

---

## ğŸ¯ Summary

This project showcases a full neural network implementation built from the ground up using only NumPy and Python. Every core concept â€” forward propagation, activation functions (ReLU & Softmax), loss calculation, backpropagation, and weight updates â€” is implemented manually to give an inside look into how deep learning works under the hood.
